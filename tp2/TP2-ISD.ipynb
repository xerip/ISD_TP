{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9k8hIyfX4Qij"
   },
   "source": [
    "# Introduction à la Science de Données\n",
    "# TP2 -  Classification par les $k$ plus proches voisins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTccv7A74Qin"
   },
   "source": [
    "La documentation Scikit-learn sur les $k$ plus proches voisins se trouve ici: http://scikit-learn.org/stable/modules/neighbors.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oWf2JPjfcbX"
   },
   "source": [
    "## Données *digits*\n",
    "Dans la première partie de ce TP, nous allons utiliser des données déjà présentes dans scikit-learn, à l'image des données Iris du premier TP.\n",
    "\n",
    "Ces données sont très connues en apprentissage, sous le noms de MNIST. Elles sont composées d'images de chiffres manuscrits à une résolution de 8*8. En scikit-learn, elles se nomment digits : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdPIwHd0gNA-"
   },
   "outputs": [],
   "source": [
    "import numpy as np # importation du package numérique\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digitsData=load_digits() # jeu de données digits\n",
    "X=digitsData.data # les exemples, un array numpy, chaque élément est aussi un array\n",
    "y=digitsData.target # les classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sIgf8FP9UMj"
   },
   "source": [
    "On peut regarder quelques informations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "PJ_o58ap9dBH",
    "outputId": "cecb6676-24cb-4659-f8b1-decb460f5764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(dtype('float64'), (1797, 64))\n",
      "(dtype('int64'), (1797,))\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype, X.shape)\n",
    "print(y.dtype, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPTbrn1niFXe"
   },
   "source": [
    "Chaque donnée est donc une image de 8 pixels par 8 pixels, en niveau de gris (256 nuances possibles), stockée sous la forme d'un vecteur de dimension 64 comme une ligne de la matrice $X$ (il y a 1797 images) et avec la valeur de la classe associée stockée dans un vecteur $y$ à part (comme pour Iris). Mais on peut quand même regarder l'image initiale :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "TDLOkUoZinAj",
    "outputId": "afeff7a5-2f1c-47b9-c6ac-bae4317e7824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Le vecteur de l'image d'indice 42 : \", array([ 0.,  0.,  0.,  0., 12.,  5.,  0.,  0.,  0.,  0.,  0.,  2., 16.,\n",
      "       12.,  0.,  0.,  0.,  0.,  1., 12., 16., 11.,  0.,  0.,  0.,  2.,\n",
      "       12., 16., 16., 10.,  0.,  0.,  0.,  6., 11.,  5., 15.,  6.,  0.,\n",
      "        0.,  0.,  0.,  0.,  1., 16.,  9.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
      "       16., 11.,  0.,  0.,  0.,  0.,  0.,  3., 16.,  8.,  0.,  0.]))\n",
      "[[ 0.  0.  0.  0. 12.  5.  0.  0.]\n",
      " [ 0.  0.  0.  2. 16. 12.  0.  0.]\n",
      " [ 0.  0.  1. 12. 16. 11.  0.  0.]\n",
      " [ 0.  2. 12. 16. 16. 10.  0.  0.]\n",
      " [ 0.  6. 11.  5. 15.  6.  0.  0.]\n",
      " [ 0.  0.  0.  1. 16.  9.  0.  0.]\n",
      " [ 0.  0.  0.  2. 16. 11.  0.  0.]\n",
      " [ 0.  0.  0.  3. 16.  8.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d185983d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # le package de visualisation\n",
    "# la ligne spéciale pour que le notebook affiche comme il faut :\n",
    "%matplotlib inline  \n",
    "\n",
    "donnee = X[42,:] # on récupère une ligne, donc une donnée\n",
    "classe = y[42]   # et sa classe\n",
    "print(\"Le vecteur de l'image d'indice 42 : \", donnee)\n",
    "\n",
    "image = np.reshape(donnee,(8,8)) # on met les 8 morceaux de taille 8 du vecteur les uns en dessous des autres\n",
    "print(image) # on affiche la matrice de pixels\n",
    "plt.imshow(image) # on affiche l'image qui lui correspond\n",
    "plt.title('Donnee numero 42, de la classe %i \\n' % classe, fontsize = 16) # avec un titre\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GvoCa-lMppar"
   },
   "source": [
    "On peut faire des affichages plus intéressant, exemple sur les 5 premières données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "id": "23nNOIYy7h1K",
    "outputId": "986f6bba-aa2f-4b7e-d496-3ff34f284feb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5wAAADiCAYAAAAicsj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAG+FJREFUeJzt3X2wZHdZJ/DvsxlQMZAZBMUNWYeAoJYlAzNiKQoBE0WhTHwJ63uGLSqpdXETV4Ho4iZr+ZLoonF9I7ORTGphC50IMy7iYkaI6ytLgsOuEJMK42ACKGBmEkiUAP72j+6ByczN3L4359xz+uTzqTp15/Y9/fRzu7+35zx9Tp+u1loAAACga/9i6AYAAACYJgMnAAAAvTBwAgAA0AsDJwAAAL0wcAIAANALAycAAAC9MHAep6purKpDQ/cBfZBvpk7GmToZZ+pkfHoeFgNnVT2qqi6pqj+uqruq6pNV9fdV9Zaq2llVm4bucUyq6lur6s+q6t75/bWnqp40dF+sTL4XV1Uvrqprq+rd8/upVdXWofvi5GR8MVW1paourqo/qKo7quofq+rWqtpVVWcM3R8PTsYXU1WPqKrXVNXNVfXRqvpEVf1NVf1WVT1j6P54cDK+flX12/Ptlb8aupf1mvyDW1VPSfJ7SZ6aZH+Sn0vy0SRfmOTsJNcm+YokrxiqxzGpqu9Icn2Sdyd5eZLTklyS5E+rakdr7YND9scDyfea/VCSr8ks3+9L8rRh22E1Mr4mX5Pk1Un+MMmvZnY/fWWSi5K8uKq+rrX23gH7YwUyviaPTLIjyZ8m+e9JPpbkXyV5SZJ3VNULWmtvG7A/ViDj61dVL0rynUn+ceheHopJD5xV9XlJ3pzkzCTf2Vp743GrXFlVX53kqze8uRGqqkck+ZUkdyT5htbax+eX/36Sm5NcnuTCwRrkAeR7XX4wyQdba5+qql+NgXPUZHzN/jrJ01pr7zv2wqr6vSQ3JPmpJN81RGOsTMbXprV2b2YD5wNU1WuS/G2SH0ti4BwRGV+/qjo1ya8n+bUk3zZwOw/J1A+pfWlmG5SvXiHgSZLW2jtba79+siJV9ayq2l1Vt1XVfVX1sar606r69hXWPaOqXltV758f6vHh+eGpFxyzTs0PK/i/81r3zA97+s350HdsvR1V9aZjDh25tar+46KHHlTV46rqy6rqtAVWf26Sf5nkmqPDZpK01g4kuTHJvz6+PwYl32vLd1prf9ta+9Qi6zIKMr6GjLfWDh0/bM4v35/krsz2djIuMr7G5/EH8eEk/5Rky0OoQT9kfP0Z/5nMdg6+ao3XG51J7+HMZ1/J3fUQ63x7ki9L8ttJ3p/kC5JckOSNVfV9rbX/kSTz4N2Q5PTMXpG4LbNDUr8qyTckuW5e71WZvdL8P5O8Jsmnkzwps1cvPifJJ+f1vjXJm5LcntlhUncl+dr5dbclOX+B3l+W5LLMDjfZvcq6R19d+vMVfvYXSZ6f2eEQ71ngdumffK8t3ywfGe8g4/ONnEcnWdr3/0yYjK8j41V1SmbD5aYkZ2S2Z/PUJG9Z5PpsKBlfX8afNb/e97TW7qmqRa42Xq21yS5J/iHJPWu8zo1JDh132eevsN6jktya5L3HXPZVSVqSV6xyG+869noPss7nJvm7JP87yabjfvYj89s5a4Hf5/L5ujsXWPdX5ut++Qo/+6H5z75p6MfV8pnHRL7XkO8Vrvur8+tuHfqxtDzoYyTjDyHjx9T4hXmNfzP0Y2o54bGR8XVkPLO99e2Y5UiSnz2+D8vwi4yvPeOZvZDy7iS/f8xlh5L81dCP53qXqR9S+5gk9zzUIm32noEknznL1hdkFvK3JfnyqnrM/Md3z78+r6q+8CQl705yelV9/UnWOSfJF2X2RurN893xj6uqx+Wzr+B90wK9X95aq9ba7tXWzex3SpJPrPCzfzpuHYYn32vLN8tHxh9ixqvqu5L8aJK3znthXGR8fRn/m/ntvzDJxfnsXqzPWUMNNoaMrz3jL0/ypUn+3YLrj97UD6m9J7PDiB6SeWB/Osm5mZ1R63ibM3v15v1V9TNJfjzJh6rqQGZnC9zTWnvnMev/RJK9Sf64qj6Y2Ss5v5fk+tba/fN1vnz+9bUnae2L1vkrPZj75l9XesL+3OPWYXjyzdTJ+EMwPxTs9Zmd9O3Fbf4yOaMi4+swHz72H/2+ql6b2R6rNyb55j5uk3WT8TWo2Rl9/1OSn26tHeyy9pCmPnD+VZLnVNWZ633QanbQ9B9kFrr/muSdmb0q8unMjsX+3hxz8qXW2qvmT3wvzOxY8ZcmeXlV/Xxr7ZXzdf68qp6c2ZPi8+bL9yZ5VVV9fWvtriRHD9Z+eZIDD9Je1x9RcrTe6UluOe5np8+/fqDj22T95Jupk/F1qqoXZLbx/Z7M3grxkPcw0AsZ70Br7eNV9cYkr6yqJ7cVTp7FYGR8bY6+T/RN8+HzqE1JHjm/7N7W2oc6vt1+DX1Mb59Lkn+f2THTP7uG69yYY44bT/L0eY3/vMK6b8gq7wHLbM/gH83X+8KTrHf0PZIvn3//HfPv/+0G3l9nz2/zJ1f42R9m9sf9iKEfV8tnHhP5fmj3n/dwjnyR8XXfb9+c2We2HUjy2KEfR8tJHysZ7+6+/OV5P189dC+WBzwuMr62++tAHvj+5JWWNw/9uK51mfp7OK/J7M3EP1ZV5660QlVtr6ofOkmNTx9d9bjrfWVmZ8w69rLT6rhTKbfW/imf3Vu4Zb7e41a4nXfNvz52/vWtmZ3m+9KqeuzxK1fV51XVqoco1NpOxfxHST6U5KU1++yfozWenuSszA5H+OQCddgY8t3N6fQZLxlfY8ar6psyO0zstiTf2Gav0jNeMr6GjFfV46vqhG3XqnpCZmcL/XicSX9sZHxtz+M/llmWj18+kuSO+b9/boE6ozLpQ2pba/dV1YsyOyZ7b1X9QWanSv6HJI/PbPf5Nyf5+ZOUuSWzJ69XVNXRs2E9NclFmR0m8Mxj1n1ekl1V9Tvz9T6eZHtmu/Lf0Vq79WjNqvqLJO/IbFf8Fye5MMn9mb1Sk9bavVX1g5ltONw6PzTg9syOUf+yzF51+fbMXgU6mYVPxdxa+2RVXZzktzI7pv2/ZfZm7x/JLOiXrXJbbCD5TrLGU41X1XOSPGf+7dEPD39ZVR2Z9/XTq9Vg48h4kjVkvKp2JNmX2UbZtUm+pY47lX5r7XWr3B4bSMaTrO15/PuSXFJVb8rsxEH3z3/XCzIbJF7aWnOuiRGR8SRr2xbfv9LlVfVfkny8tXb9Krc1TkPvYt2IJbOzWP1Ikj9Jcjizz9b5+8zC/wNJTjlm3Rtz4qmYvyTJnsyGrvuS/J/MAnZ5jtmNn9nn97wmsz+Me5LcO//3TyU57Zh6l2Z2iuUPZ3ZG2Dvm9Z+5Qu9fmeR1mb138v5533+W5CezwKFSWd/pxl+U2edu3je/v65P8uShH0eLfK9w/TXl+5j1V1yGfiwtMn6SzK6a8SQ7T5ZvGR/vIuMLZ3x7ZifCuj2zQeL+eW+/leTrhn4cLTL+IL/7whk/SY1DWeKPRan5LwEAAACdmvp7OAEAABiIgRMAAIBeGDgBAADohYETAACAXhg4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBeGDgBAADohYETAACAXhg4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBeGDgBAADohYETAACAXhg4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBeGDgBAADohYETAACAXhg4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBebOqjaFW1Puo+VFu2bOm03umnn95ZrXvuuaezWh/4wAc6q/XpT3+6s1o9+Ghr7fEbfaNjzXfXnvrUp3ZWa9Om7p5qusz33Xff3VmtHgyS7+Thk/FTTz21s1pPecpTOqt13333dVbrtttu66xWD2T8OE94whM6rdfldsonPvGJzmrdcsstndWynXKisea7a6ecckpntbZu3dpZrfe9732d1Rq5hfPdy8A5VmeffXan9a644orOau3fv7+zWpdeemlntQ4fPtxZrR68f+gGpmzXrl2d1dq8eXNntS677LLOau3bt6+zWj2Q757t2LGjs1p79+7trNaBAwc6q3XWWWd1VqsHMn6cCy64oNN6XW6nHDx4sLNaXf7t2U55+Hr0ox/dWa1Xv/rVndU677zzOqs1cgvn2yG1AAAA9MLACQAAQC8MnAAAAPTCwAkAAEAvFho4q+oFVXVrVd1eVd2dkQZGQsaZMvlm6mScqZNxltmqA2dVnZLk15J8S5KvSPI9VfUVfTcGG0XGmTL5ZupknKmTcZbdIns4n5Xk9tbawdba/UnekOTcftuCDSXjTJl8M3UyztTJOEttkYHz9CR3HPP9nfPLHqCqLqyqm6rqpq6agw2yasblmyXmOZypk3GmznYKS23TAuvUCpe1Ey5obVeSXUlSVSf8HEZs1YzLN0vMczhTJ+NMne0UltoiezjvTHLGMd8/MckH+2kHBiHjTJl8M3UyztTJOEttkYHznUm+tKqeVFWPTPLdSX6337ZgQ8k4UybfTJ2MM3UyzlJb9ZDa1tqnquplSd6a5JQkr22tvaf3zmCDyDhTJt9MnYwzdTLOslvkPZxprb0lyVt67gUGI+NMmXwzdTLO1Mk4y2yRQ2oBAABgzQycAAAA9MLACQAAQC8MnAAAAPRioZMGTcUVV1zRab0zzzyzs1pbtmzprNZdd93VWa0Xv/jFndVKkj179nRaj/4cOXKks1rPfe5zO6v1vOc9r7Na+/bt66wW/du2bVun9d7+9rd3Vuvuu+/urNbWrVs7q0X/uty2OP/88zurlSQXXXRRZ7Wuvvrqzmpt3769s1r79+/vrBbLZefOnZ3VOnDgQGe1OJE9nAAAAPTCwAkAAEAvDJwAAAD0wsAJAABALwycAAAA9MLACQAAQC8MnAAAAPTCwAkAAEAvDJwAAAD0wsAJAABALwycAAAA9MLACQAAQC8MnAAAAPTCwAkAAEAvDJwAAAD0wsAJAABALwycAAAA9MLACQAAQC82Dd3AarZv395ZrTPPPLOzWkny5Cc/ubNaBw8e7KzWDTfc0FmtLu//JNmzZ0+n9fisbdu2dVrvrLPO6rReVw4cODB0CwzkvPPO67Teu9/97s5q7d27t7Nal112WWe16N+uXbs6q3XllVd2VitJbrrpps5qdbmdsn///s5qsTw2b97cab2dO3d2Vuuqq67qrNbWrVs7q9W1Q4cODXK79nACAADQCwMnAAAAvTBwAgAA0AsDJwAAAL0wcAIAANCLVQfOqjqjqt5eVbdU1Xuq6uKNaAw2iowzZfLN1Mk4UyfjLLtFPhblU0l+tLX2rqp6dJKbq+qG1tp7e+4NNoqMM2XyzdTJOFMn4yy1VfdwttY+1Fp71/zfH0tyS5LT+24MNoqMM2XyzdTJOFMn4yy7Nb2Hs6q2JnlGknf00QwMTcaZMvlm6mScqZNxltEih9QmSarq1CS/k+SS1to9K/z8wiQXdtgbbKiTZVy+WXaew5k6GWfqbKewrBYaOKvqEZkF/PWttTeutE5rbVeSXfP1W2cdwgZYLePyzTLzHM7UyThTZzuFZbbIWWoryW8muaW19ov9twQbS8aZMvlm6mScqZNxlt0i7+F8dpIfSPL8qjowX761575gI8k4UybfTJ2MM3UyzlJb9ZDa1tqfJKkN6AUGIeNMmXwzdTLO1Mk4y25NZ6kFAACARRk4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBerHqW2qFt2bKls1o333xzZ7WS5ODBg53W60rXvyf9ueSSSzqrdfnll3dWK0lOO+20Tut15cYbbxy6BQZy1VVXdVrv0KFDndXqsrd9+/Z1Vov+dbktcOaZZ3ZWq+t6+/fv76xWl9t2hw8f7qwW/dq5c2en9bZu3dpZrd27d3dWq8v/D44cOdJZraT7bcVF2cMJAABALwycAAAA9MLACQAAQC8MnAAAAPTCwAkAAEAvDJwAAAD0wsAJAABALwycAAAA9MLACQAAQC8MnAAAAPTCwAkAAEAvDJwAAAD0wsAJAABALwycAAAA9MLACQAAQC8MnAAAAPTCwAkAAEAvDJwAAAD0YtPQDaxmy5YtndXav39/Z7XGrMv77PDhw53V4kRXXXVVZ7V2797dWa1kvI/95s2bh26BNejy8brkkks6q5Uk5513Xqf1urJz586hW2AgBw8e7LTeYx/72M5q3XDDDaOsdc4553RWKxnv/31DOffcczur9Uu/9Eud1UqS6667rtN6Xbn44os7q/WSl7yks1pDsocTAACAXhg4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBeLDxwVtUpVfWXVfXmPhuCIcg3UyfjTJ2MM3UyzrJayx7Oi5Pc0lcjMDD5ZupknKmTcaZOxllKCw2cVfXEJC9Mck2/7cDGk2+mTsaZOhln6mScZbboHs6rkrwiyT8/2ApVdWFV3VRVN3XSGWwc+WbqZJypk3Gm7qQZl2/GbNWBs6pelOTDrbWbT7Zea21Xa21Ha21HZ91Bz+SbqZNxpk7GmbpFMi7fjNkiezifneTbqupQkjckeX5Vva7XrmDjyDdTJ+NMnYwzdTLOUlt14Gyt/Xhr7Ymtta1JvjvJ21pr3997Z7AB5Jupk3GmTsaZOhln2fkcTgAAAHqxaS0rt9ZuTHJjL53AwOSbqZNxpk7GmToZZxnZwwkAAEAvDJwAAAD0wsAJAABALwycAAAA9GJNJw0awuHDhzurtX379s5qdW3Lli2d1ery99yzZ09ntaAL27Zt66zWgQMHOqvFyi6//PLOal188cWd1eraeeed11mtI0eOdFaLh7cut6HOOeeczmpdffXVndV65Stf2VmtJLn00ks7rbfs7r777lHWSpILLrigs1pdblt0ae/evUO30Al7OAEAAOiFgRMAAIBeGDgBAADohYETAACAXhg4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBeGDgBAADohYETAACAXhg4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBeGDgBAADohYETAACAXhg4AQAA6IWBEwAAgF5sGrqB1Rw8eLCzWtu3b++sVpKcf/75o6zVpSuvvHLoFoAltnv37s5qnXXWWZ3VSpKnP/3pndXau3dvZ7X27dvXWa1rr722s1pJt71xoiuuuKLTevv37++s1pYtWzqrdfbZZ3dWa8+ePZ3V4kQ33nhjZ7U2b97cWa0k2bZtW2e1uvw9r7vuus5qHTlypLNaQ7KHEwAAgF4YOAEAAOiFgRMAAIBeGDgBAADohYETAACAXiw0cFbV5qq6vqr+uqpuqaqv7bsx2EgyzpTJN1Mn40ydjLPMFv1YlF9O8r9aa99VVY9M8qgee4IhyDhTJt9MnYwzdTLO0lp14KyqxyR5TpKdSdJauz/J/f22BRtHxpky+WbqZJypk3GW3SKH1J6Z5CNJrq2qv6yqa6rq83vuCzaSjDNl8s3UyThTJ+MstUUGzk1JnpnkN1prz0hyb5JLj1+pqi6sqpuq6qaOe4S+rZpx+WaJeQ5n6mScqbOdwlJbZOC8M8mdrbV3zL+/PrPQP0BrbVdrbUdrbUeXDcIGWDXj8s0S8xzO1Mk4U2c7haW26sDZWvu7JHdU1dPmF31jkvf22hVsIBlnyuSbqZNxpk7GWXaLnqX2h5O8fn5WrINJXtJfSzAIGWfK5Jupk3GmTsZZWgsNnK21A0nsomeyZJwpk2+mTsaZOhlnmS3yHk4AAABYMwMnAAAAvTBwAgAA0AsDJwAAAL0wcAIAANCLRT8WZTAHDx7srNall17aWa0kueKKKzqrdfPNN3dWa8cOJzF7ODpy5Ein9fbt29dZrXPPPbezWmeddVZntXbv3t1ZLVZ24MCBzmpt27ats1pd17v88ss7q9Xl38uhQ4c6q5V0+7zAiQ4fPtxpvauvvrrTel3Zs2dPZ7UuuuiizmqxXLrc7jnttNM6q2Xb4kT2cAIAANALAycAAAC9MHACAADQCwMnAAAAvTBwAgAA0AsDJwAAAL0wcAIAANALAycAAAC9MHACAADQCwMnAAAAvTBwAgAA0AsDJwAAAL0wcAIAANALAycAAAC9MHACAADQCwMnAAAAvTBwAgAA0AsDJwAAAL2o1lr3Ras+kuT9q6z2uCQf7fzGuzHW3sbaVzJMb1/SWnv8Bt/movlOxvt4jbWvZLy9PWzynch4z8bam4yfaKyPVTLe3sbaV/IwyvgEnsOT8fY21r6Sje9t4Xz3MnAudMNVN7XWdgxy46sYa29j7SsZd29DGet9Mta+kvH2Nta+hjbW+2WsfSXj7W2sfQ1pzPfJWHsba1/JuHsbypjvk7H2Nta+knH35pBaAAAAemHgBAAAoBdDDpy7Brzt1Yy1t7H2lYy7t6GM9T4Za1/JeHsba19DG+v9Mta+kvH2Nta+hjTm+2SsvY21r2TcvQ1lzPfJWHsba1/JiHsb7D2cAAAATJtDagEAAOjFIANnVb2gqm6tqtur6tIhejheVZ1RVW+vqluq6j1VdfHQPR2rqk6pqr+sqjcP3cuxqmpzVV1fVX89v+++duiehjbGfCcyvl4yfiIZXx8ZXx5jzPjY852MM+PyvTIZX7sx5jtZjoxv+CG1VXVKktuSnJPkziTvTPI9rbX3bmgjJ/b1xUm+uLX2rqp6dJKbk5w3dF9HVdV/SLIjyWNaay8aup+jquq6JH/cWrumqh6Z5FGttSND9zWUseY7kfH1kvEHkvH1k/HlMNaMjz3fyTgzLt8nkvH1GWO+k+XI+BB7OJ+V5PbW2sHW2v1J3pDk3AH6eIDW2odaa++a//tjSW5JcvqwXc1U1ROTvDDJNUP3cqyqekyS5yT5zSRprd0/toAPYJT5TmR8PWR8RTK+DjK+VEaZ8THnOxlnxuX7Qcn4Go0x38nyZHyIgfP0JHcc8/2dGUmYjqqqrUmekeQdw3byGVcleUWSfx66keOcmeQjSa6dH2JwTVV9/tBNDWz0+U5kfA1k/EQyvj4yvjxGn/ER5jsZZ8ble2UyvnZjzHeyJBkfYuCsFS4bzalyq+rUJL+T5JLW2j0j6OdFST7cWrt56F5WsCnJM5P8RmvtGUnuTTKK9wEMaNT5TmR8jWT8RDK+9n5kfLmMOuNjy3cy6ozL98pkfG39jDXfyZJkfIiB884kZxzz/ROTfHCAPk5QVY/ILOCvb629ceh+5p6d5Nuq6lBmhzw8v6peN2xLn3Fnkjtba0dffbo+s9A/nI0234mMr4OMn0jG107Gl8toMz7SfCfjzbh8r0zG12as+U6WJONDDJzvTPKlVfWk+RtbvzvJ7w7QxwNUVWV2/PMtrbVfHLqfo1prP95ae2JrbWtm99XbWmvfP3BbSZLW2t8luaOqnja/6BuTjOKN3QMaZb4TGV8PGV+RjK+RjC+dUWZ8rPlOxptx+X5QMr4GY813sjwZ37TRN9ha+1RVvSzJW5OckuS1rbX3bHQfK3h2kh9I8v+q6sD8sp9orb1lwJ6WwQ8nef38CetgkpcM3M+gRpzvRMbXS8aPIeOTJOPHGHHG5Xt95Ps4Mj45o8/4hn8sCgAAAA8PQxxSCwAAwMOAgRMAAIBeGDgBAADohYETAACAXhg4AQAA6IWBEwAAgF4YOAEAAOiFgRMAAIBe/H9PWhnyep27MAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d10d38810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d10d38690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "for index, (image, classe) in enumerate(zip(X[0:5], y[0:5])): # zip forme des couples (X,Y)\n",
    "    plt.subplot(1, 5, index + 1) #1 et 5 = taille image, index+1 est l'endroit ou il est affiché\n",
    "    plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "    plt.title('Classe : %i\\n' % classe, fontsize = 18)\n",
    "plt.show()\n",
    "    \n",
    "plt.figure(figsize=(16,4))\n",
    "for index, (image, classe) in enumerate(zip(X[42:47], y[42:47])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (8,8)), cmap=\"copper\")\n",
    "    plt.title('Classe : %i\\n' % classe, fontsize = 18)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ci58h2vz4Qiq"
   },
   "source": [
    "## Création et entraînement d'un classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efNARzTG4Qiv"
   },
   "source": [
    "Notre objectif est maintenant d'apprendre, sur la base d'un échantillon d'images \"chiffres\", un classifieur capable de prédire le chiffre qui correspond à une nouvelle image. Nous allons utiliser la méthode des $k$-plus proches voisins pour cet apprentissage. Elle est implémentée dans un package appelé *neighbors*. Examinons la série d'instructions suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5646
    },
    "colab_type": "code",
    "id": "NyjLe8Ai4Qiy",
    "outputId": "7480e968-21fe-4401-d027-dc7cd9fbe9b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsClassifier in module sklearn.neighbors.classification:\n",
      "\n",
      "class KNeighborsClassifier(sklearn.neighbors.base.NeighborsBase, sklearn.neighbors.base.KNeighborsMixin, sklearn.neighbors.base.SupervisedIntegerMixin, sklearn.base.ClassifierMixin)\n",
      " |  Classifier implementing the k-nearest neighbors vote.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, optional (default = 5)\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : str or callable, optional (default = 'uniform')\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, optional (default = 30)\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : integer, optional (default = 2)\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : string or callable, default 'minkowski'\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of the DistanceMetric class for a\n",
      " |      list of available metrics.\n",
      " |  \n",
      " |  metric_params : dict, optional (default = None)\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, optional (default = 1)\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      " |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
      " |  >>> neigh.fit(X, y) # doctest: +ELLIPSIS\n",
      " |  KNeighborsClassifier(...)\n",
      " |  >>> print(neigh.predict([[1.1]]))\n",
      " |  [0]\n",
      " |  >>> print(neigh.predict_proba([[0.9]]))\n",
      " |  [[ 0.66666667  0.33333333]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RadiusNeighborsClassifier\n",
      " |  KNeighborsRegressor\n",
      " |  RadiusNeighborsRegressor\n",
      " |  NearestNeighbors\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances\n",
      " |     but different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsClassifier\n",
      " |      sklearn.neighbors.base.NeighborsBase\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.neighbors.base.KNeighborsMixin\n",
      " |      sklearn.neighbors.base.SupervisedIntegerMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the class labels for the provided data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape [n_samples] or [n_samples, n_outputs]\n",
      " |          Class labels for each data sample.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test data X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          of such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. Classes are ordered\n",
      " |          by lexicographic order.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors to get (default is the value\n",
      " |          passed to the constructor).\n",
      " |      \n",
      " |      return_distance : boolean, optional. Defaults to True.\n",
      " |          If False, distances will not be returned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dist : array\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      ind : array\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NeighborsClassifier\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n",
      " |      (array([[ 0.5]]), array([[2]]...))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors for each sample.\n",
      " |          (default is value passed to the constructor).\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, optional\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]\n",
      " |          n_samples_fit is the number of samples in the fitted data\n",
      " |          A[i, j] is assigned the weight of edge that connects i to j.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[ 1.,  0.,  1.],\n",
      " |             [ 0.,  1.,  1.],\n",
      " |             [ 1.,  0.,  1.]])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.SupervisedIntegerMixin:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model using X as training data and y as target values\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, BallTree, KDTree}\n",
      " |          Training data. If array or matrix, shape [n_samples, n_features],\n",
      " |          or [n_samples, n_samples] if metric='precomputed'.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix}\n",
      " |          Target values of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors as nn # importation du package d'algorithmes travaillant sur les points voisins\n",
    "help(nn.KNeighborsClassifier) # que fait cette instruction qui sera très utile par la suite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHnrWhos4Qi9"
   },
   "source": [
    "Sympa, non ? Ce type d'instruction est utilisable pour toute classe de Python. \n",
    "\n",
    "Continuons l'exploration des $k$ plus proches voisins. Dans la série d'instructions suivante, on indique comment un classifieur peut être appris à partir de données étiquetées, et comment réaliser la prédiction sur un nouvel exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOZ0WxXZ4QjO"
   },
   "source": [
    "Les fonctions *predict* et *fit* existent **pour tous les classifieurs** disponibles dans scikit-learn.\n",
    "\n",
    "On note ici la syntaxe de la fonction *predict* : on lui passe en réalité un tableau d'exemples (ici, un tableau avec un seul exemple constitué de 64 attributs), et elle renvoit un tableau contenant la classe prédite pour chaque exemple du tableau en paramètre. Evidemment, dans les tableaux en entrée et en sortie, les indices des classes prédites correspondent aux indices des exemples en entrée ! \n",
    "\n",
    "Ainsi, lorsque l'on sait que l'on n'applique predict qu'à un seul exemple, une sélection finale [0] comme ci-après renvoit la première (et la seule) composante du tableau de résultat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "abQTK7Mm4QjH",
    "outputId": "04aa51cc-e3ca-42d9-ffc8-a9b1d632294b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pr\\xc3\\xa9diction pour le nouvel exemple: ', array([1]))\n"
     ]
    }
   ],
   "source": [
    "nb_voisins = 15 # on fixe le nombre de voisins, à partir de 2 et au max le nombre d'exemples dans le jeu de données\n",
    "clf = nn.KNeighborsClassifier(nb_voisins) \n",
    "# ci-dessus, création d'un classifieur: la variable clf est un \"objet\" classifieur, vide pour l'instant \n",
    "#print(clf) # le classifieur est vide pour l'instant, il n'a pas été entraîné sur des données\n",
    "clf.fit(X, y) # entraînement du classifieur clf sur les données étiquetées\n",
    "nouvel_ex = np.random.randint(0,256,64,int)\n",
    "print('prédiction pour le nouvel exemple: ',\n",
    "      clf.predict(nouvel_ex.reshape(1,-1))) # prédiction du modèle appris sur la description d'un chiffre aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9zal_50w4QjR",
    "outputId": "7e81f630-fbf9-4217-b3dc-c9ec48b52f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pr\\xc3\\xa9diction pour le nouvel exemple: ', 8)\n"
     ]
    }
   ],
   "source": [
    "print('prédiction pour le nouvel exemple: ', clf.predict(nouvel_ex.reshape(1,-1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FymLN7OV4Qja"
   },
   "source": [
    "Pour certains types de classifieurs, on peut même récupérer la probabilité que le classifieur attribue à l'appartenance de l'exemple à chaque classe possible. La fonction *predict_proba* fonctionne comme la fonction *predict*, sauf que le tableau en sortie contient, pour chaque exemple du tableau en entrée, un tableau de probabilité de la même taille que le nombre de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "Lnzq5Zut4Qjf",
    "outputId": "e6e6ff86-aa08-4476-9276-5ff4f7b93e73",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.73333333 0.         0.         0.         0.06666667\n",
      " 0.2        0.         0.         0.        ]\n",
      "[0.26666667 0.33333333 0.         0.         0.         0.\n",
      " 0.06666667 0.         0.26666667 0.06666667]\n"
     ]
    }
   ],
   "source": [
    "autre_ex=np.random.randint(0,256,64,int) # on génère un autre exemple aléatoirement, avec 64 attributs prenant leurs valeurs entre 0 et 255\n",
    "print(clf.predict_proba(nouvel_ex.reshape(1,-1))[0]) # probabilité d'appartenance à chaque classe pour ce chiffre\n",
    "print(clf.predict_proba(autre_ex.reshape(1,-1))[0]) # idem pour un autre exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SJgdW_NF4Qjn"
   },
   "source": [
    "Ci-dessus, le premier nouvel exemple est classé dans la classe d'indice 7, pour laquelle la probabilité est la plus grande.\n",
    "\n",
    "A votre avis, quelle classe sera attribuée au deuxième exemple, et pourquoi ? Indiquez ci-après l'instruction à exécuter pour vérifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwmb39cy4Qjs"
   },
   "outputs": [],
   "source": [
    "# La classe qui sera attribué au deuxième exemple est la classe 1 parce qu'elle a la plus grande probabilité \n",
    "# dans le tableau de probabilité (indice 1) renvoyé par la commande predict_proba(autre_ex.reshape(1,-1))[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsSaiAzO4Qj9"
   },
   "source": [
    "Une première façon d'évaluer la qualité d'un classifieur est de le tester sur les exemples qui ont servi à l'apprendre. On utilise du coup la même fonction *predict*, appliquée au tableau des exemples d'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "at1QD73r4Qj_",
    "outputId": "e839e11c-e7a3-4702-a4d9-2706d4faa035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. 12. ... 10.  0.  0.]\n",
      " [ 0.  0.  6. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 16. 12.  0.]\n",
      " ...\n",
      " [ 0.  0.  7. ...  0.  0.  0.]\n",
      " [ 0.  1.  7. ...  6.  0.  0.]\n",
      " [ 0.  0.  5. ...  3.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "Z = clf.predict(X) # vecteur des classes prédites pour chaque exemple de l'ensemble d'apprentissage\n",
    "print(X[Z!=y]) # le tableau d'exemples pour lesquels la prédiction a été mauvaise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkfXE6dx4QkF"
   },
   "source": [
    "Indiquez ci-après les instructions qui permettent (1) de connaître les probabilités d'appartenance des exemples mal classés à chacune des 10 classes de *digit*, et (2) les vraies classes de ces exemples (pour cela, on peut utiliser la fonction *numpy.argwhere* avec un peu de jugeotte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZATD9txd4QkJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.06666667 0.         0.06666667\n",
      "  0.         0.         0.         0.86666667]\n",
      " [0.         0.06666667 0.         0.06666667 0.2        0.\n",
      "  0.         0.26666667 0.13333333 0.26666667]\n",
      " [0.         0.46666667 0.46666667 0.         0.         0.\n",
      "  0.         0.         0.06666667 0.        ]\n",
      " [0.         0.46666667 0.         0.         0.46666667 0.06666667\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.8        0.         0.         0.         0.\n",
      "  0.         0.         0.2        0.        ]\n",
      " [0.         0.6        0.         0.         0.         0.\n",
      "  0.         0.         0.4        0.        ]\n",
      " [0.         0.         0.         0.66666667 0.         0.\n",
      "  0.         0.         0.33333333 0.        ]\n",
      " [0.         0.         0.         0.13333333 0.         0.33333333\n",
      "  0.         0.         0.         0.53333333]\n",
      " [0.         0.33333333 0.         0.         0.06666667 0.\n",
      "  0.33333333 0.         0.26666667 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.13333333\n",
      "  0.         0.         0.6        0.26666667]\n",
      " [0.         0.53333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.46666667]\n",
      " [0.         0.         0.         0.         0.26666667 0.\n",
      "  0.         0.73333333 0.         0.        ]\n",
      " [0.         0.86666667 0.         0.         0.         0.\n",
      "  0.         0.         0.13333333 0.        ]\n",
      " [0.         0.         0.26666667 0.13333333 0.         0.\n",
      "  0.         0.6        0.         0.        ]\n",
      " [0.         0.         0.         0.4        0.         0.\n",
      "  0.2        0.         0.33333333 0.06666667]\n",
      " [0.         0.6        0.06666667 0.         0.         0.\n",
      "  0.         0.         0.33333333 0.        ]\n",
      " [0.         0.         0.         0.13333333 0.         0.\n",
      "  0.         0.8        0.         0.06666667]\n",
      " [0.         0.         0.         0.         0.         0.4\n",
      "  0.6        0.         0.         0.        ]\n",
      " [0.         0.86666667 0.         0.         0.         0.\n",
      "  0.         0.         0.13333333 0.        ]\n",
      " [0.         0.06666667 0.         0.         0.         0.53333333\n",
      "  0.         0.         0.         0.4       ]\n",
      " [0.         0.         0.         0.06666667 0.2        0.\n",
      "  0.         0.26666667 0.33333333 0.13333333]\n",
      " [0.         0.         0.         0.73333333 0.         0.\n",
      "  0.         0.         0.13333333 0.13333333]\n",
      " [0.         0.         0.         0.         0.2        0.\n",
      "  0.         0.33333333 0.33333333 0.13333333]\n",
      " [0.         0.         0.         0.         0.         0.8\n",
      "  0.         0.         0.         0.2       ]\n",
      " [0.         0.         0.         0.46666667 0.         0.53333333\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.6        0.         0.         0.         0.\n",
      "  0.         0.         0.4        0.        ]]\n",
      "[[5]\n",
      " [9]\n",
      " [2]\n",
      " [4]\n",
      " [8]\n",
      " [8]\n",
      " [8]\n",
      " [5]\n",
      " [6]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [8]\n",
      " [2]\n",
      " [8]\n",
      " [8]\n",
      " [3]\n",
      " [5]\n",
      " [8]\n",
      " [9]\n",
      " [4]\n",
      " [9]\n",
      " [4]\n",
      " [9]\n",
      " [3]\n",
      " [8]]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict_proba(X[Z!=y])) # Z = classes prédites sur chacune des données de X\n",
    "#vraies classes \n",
    "print(y[np.argwhere(Z!=y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7n9FWWs4QkV"
   },
   "source": [
    "Chaque classifieur possède une fonction *score*, qui permet de comparer les prédictions d'un ensemble d'exemples $X$ pour lesquelles on connaît les étiquettes $y$ : la fonction calcule le taux de bonne classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UJsi-fuG4QkY",
    "outputId": "a2c20e7f-8881-4e12-ac8d-f40fa08f7966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('taux de bonne classification', 0.9855314412910406)\n"
     ]
    }
   ],
   "source": [
    "print('taux de bonne classification', clf.score(X,y)) # taux de bonne classification du modèle sur l'ensemble d'apprentissage: fonction score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2I8C2sn4Qkd"
   },
   "source": [
    "On la détourne facilement pour obtenir le taux d'erreur : faites le (vous devez obtenir 0.01446855...)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3kjUGxv4Qki"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"taux d'erreur classification\", 0.014468558708959356)\n"
     ]
    }
   ],
   "source": [
    "print('taux d\\'erreur classification', 1-clf.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ag7_Jwgr4Qk6"
   },
   "source": [
    "## Variation du nombre de voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GE1r3Yq4Qk7"
   },
   "source": [
    "L'algorithme des $k$-plus proches voisins fonctionne avec plusieurs hyper-paramètres (paramètres de l'agorithme, pas du modèle appris): la valeur de $k$ est un de ces paramètres. Réalisez un programme qui fait varier cet hyper-paramètres dans un intervalle comprenant des valeurs entre 2 et 15, et stocker l'évolution de l'erreur d'apprentissage (celle calculée sur l'échantillon d'apprentissage), puis en réaliser une courbe avec en abscisse les valeurs de k, et en ordonnées les erreurs.\n",
    "\n",
    "On peut utiliser pour ce faire la fonction de construction d'un tableau *numpy.arange* (cf documentation), la fonction *len(X)* qui renvoit la taille d'un tableau à une dimension. Pour la courbe, on utilisera simplement *plot(abs, ord)* du package *pyplot* de *matplotlib*, comme vue au premier TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RK49MqBj4Qk9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.008903728436282732, 0.006677796327211993, 0.007790762381747363, 0.009460211463550361, 0.01001669449081799, 0.009460211463550361, 0.01057317751808573, 0.01112966054535336, 0.014468558708959356, 0.011686143572620988, 0.012242626599888728, 0.013355592654423987, 0.013912075681691727, 0.014468558708959356]\n"
     ]
    }
   ],
   "source": [
    "tab=[]\n",
    "for i in np.arange(2,16):\n",
    "    clf = nn.KNeighborsClassifier(i)\n",
    "    clf.fit(X,y)\n",
    "    tab.append(1-clf.score(X,y))\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FKPvlY14QlQ"
   },
   "source": [
    "Qu'observez-vous ? A quelle valeur de k atteint-on un meilleur classifieur ? Quelle est globalement, sur ce jeu de données, l'influence de $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6bMmcGP4QlR"
   },
   "source": [
    "## Evaluation de l'erreur réelle du classifieur appris "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8h8dMnM4QlT"
   },
   "source": [
    "Lorsque le score du classifieur appris est évalué sur l'ensemble d'apprentissage, il est en général sur-estimé (pourquoi ?) et donc, très peu fiable. La meilleure méthode pour évaluer un classifieur consiste à calculer son score sur un échantillon test, indépendant de l'échantillon d'apprentissage mais généré dans les mêmes\n",
    "conditions. Lorsqu'on dispose d'un seul ensemble d'exemples (comme c'est le cas de *digits*, il faut donc:\n",
    "\n",
    "* répartir les données en un sous-ensemble d'apprentissage et un sous-ensemble test,\n",
    "* entrainer un classifieur sur l'ensemble d'apprentissage \n",
    "* évaluer ce classifieur sur l'ensemble test (on a ici une évaluation de l'erreur réelle, qui reste instable puisque dépend du découpage effectué)\n",
    "\n",
    "Si les données sont peu nombreuses, comme c'est le cas pour le jeu de données *digits*, cette évaluation risque d'être pessimiste (avez-vous une idée de pourquoi ? Si oui, expliquez, sinon réfléchissez!).\n",
    "\n",
    "Scikit-learn vient avec toute une panoplie d'outils pour évaluer cette erreur. Pour l'instant, nous n'utiliserons que la fonction qui permet de diviser un échantillon en deux parties (attributs et classes): c'est la fonction *train_test_split* du package *model_selection*, que nous appliquons ci-après sur Iris (nous ne printons que les trois premiers exemples de chaque sous-échantillon, avec leurs étiquettes):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "n_FJ2xiw4QlT",
    "outputId": "ba0e0283-25f5-4349-a469-665c3a5738c3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# production de deux sous-échantillon\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.25, random_state=42) \n",
    "\n",
    "print(Xtrain[:3,:], ytrain[:3])\n",
    "print(Xtest[:3,:], ytest[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "56FteqCq4QlZ"
   },
   "source": [
    "Ici, nous produisons un découpage dans lequel l'ensemble d'apprentissage représente 60% de l'échantillon initial, et l'échantillon de test reprend 40% des données initiales.\n",
    "\n",
    "En vous inspirant de ce mode de découpage, écrire une séquence d'instructions permettant de séparer *digits* en deux parties égales, d'apprendre un 3-plus proches voisins sur le premier sous-échantillon, et de le tester sur le second: vous obtenez une **estimation** de l'erreur réelle. Obtenez-vous la même erreur que celle d'apprentissage mesurée précédemment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8kQdGIU4QlZ"
   },
   "outputs": [],
   "source": [
    "# a vous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6f608IhM4Qlo"
   },
   "source": [
    "Faites maintenant à nouveau varier $k$, et pour chaque valeur, indiquez l'erreur réelle estimée sur la base d'un train_test_split de 70%, 30% ; tracer la courbe. Observez-bien les différences de valeurs des erreurs d'apprentissage et réelle: pourquoi sont-elles différentes ? Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4T5bH6HG4Qlp"
   },
   "outputs": [],
   "source": [
    "# a vous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtuiGNxH4Ql8"
   },
   "source": [
    "## Variation autour de la métrique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpHiq-m24Ql9"
   },
   "source": [
    "Au delà du nombre de voisins, un autre hyper-paramètre est la métrique utilisée pour calculer la distance entre les exemples. Par défaut, la distance de Minkowski est utilisée, avec le paramètre $p=2$ qui indique que nous considérons la distance euclidienne. Avec $p=1$, nous aurions la distance de manhattan, et de façon générale, avec p>0, la distance utilisée est $l_p$ :\n",
    "\n",
    "$$l_p(x, x')=(\\sum_{i=1}^n |x_i - x'_i|^p)^{\\frac{1}{p}}$$\n",
    "\n",
    "Ecrire un programme permettant de faire varier la distance utilisée pour évaluer son impact sur les performances, en faisant aussi varier $k$. Tracez les 3 courbes sur un même plot (cf. doc de *plot* pour voir comment faire), une pour chaque valeur de $p$ parmi ${1,2,5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtpJAvCs4QmA"
   },
   "outputs": [],
   "source": [
    "# a vous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpZryjjk4QmI"
   },
   "source": [
    "# Analyse en composantes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bl-sB9fd4QmJ"
   },
   "source": [
    "## Un exemple sur Iris "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1lCUg4w4QmK"
   },
   "source": [
    "l'ACP est une méthode classique des statistiques qui consiste à transformer des variables liées entre elles en nouvelles variables décorrélées les unes des autres. \n",
    "Ces nouvelles variables sont les composantes principales. L'ACP permet de réduire le nombre de variables décrivant le problème et de rendre l'information moins redondante.\n",
    "\n",
    "En apprentissage automatique, l'ACP est parfois naturellement utilisée en pré-traitement des données, notamment pour réduire les dimensions, les décorréler ou les débruiter.\n",
    "Nous illustrons ici l'ACP à des fins de visualisation des données~: nous ne gardons que les deux composantes principales.\n",
    "\n",
    "L'ACP est disponile sous sklearn dans le package decomposition. L'exemple ci-dessous illustre l'application d'une ACP au jeu de données *Iris*, et sa visualisation. Comme évoqué en cours, l'ACP réalise une centralisation (et parfois une réduction) des données d'entrée, lors du calcul des composantes principaux. Il est important de comprendre cette petite astuce pour pouvoir être capable de faire cette projection manuellement si besoin. Le mode de fonctionnement de cette projection après centralisation est illustré au milieu du programme.\n",
    "\n",
    "A la fin, un mode d'affichage des données transformées est proposé.\n",
    "\n",
    "Comprendre ce programme et l'exécuter. La séparation des classes vous semble-t-elle plus facile dans l'espace des composantes principales ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "N5Cs1DoY4QmK",
    "outputId": "abd386db-6cc2-4868-ebe9-3b92e9dec6a8"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "FIG_SIZE = (20, 14)\n",
    "COLORS = ['blue', 'red', 'green']\n",
    "SYMBOLS = ['^', 's', '+']\n",
    "\n",
    "X,y = load_iris(return_X_y=True)\n",
    "\n",
    "# Transformation des données par ACP\n",
    "pca = PCA(n_components=2) # creation d'un objet acp, on ne garde que les deux premières composantes principales\n",
    "pca.fit(X) # calcul des composantes principales\n",
    "\n",
    "# extraire les composantes principales, puis afficher le dataset selon elles\n",
    "X_pca = pca.transform(X) # projection des données sur le nouvel espace 2D\n",
    "\n",
    "#################### Illustration de la nécessaire centralisation des données pour la PCA\n",
    "print('\\nPremiere CP1 = ', pca.components_[0]) # obtention de la PCA1 par transformation des variables initiales\n",
    "print('Seconde CP2 = ', pca.components_[1])\n",
    "print('Moyennes des variables selon ACP = ',pca.mean_)\n",
    "print('Moyenne de la premiere variable dans echantillon = ',np.mean(X[:,0]))\n",
    "print('Premier point IRIS ', X[0])\n",
    "pointrecentre = X[0]-pca.mean_\n",
    "print('Premier point IRIS recentre = ', pointrecentre )\n",
    "# verification du point transforme: on fait le calcul à la main\n",
    "point = np.dot(pointrecentre, np.transpose(pca.components_)) # mode de calcul du premier individu\n",
    "print('Transformation manuelle = ', point)\n",
    "print('Transformation acp = ', X_pca[0])  \n",
    "####################################################################\n",
    "\n",
    "# Visualisation des points projetés dans l'espace des deux premières composantes principales\n",
    "for l, c, m in zip(range(0, 3), COLORS, SYMBOLS):\n",
    "    plt.scatter(X_pca[y == l, 0], X_pca[y == l, 1],\n",
    "                color=c,\n",
    "                label='class %s' % l,\n",
    "                alpha=0.5,\n",
    "                marker=m\n",
    "                )\n",
    "plt.xlabel('premiere composante principale')\n",
    "plt.ylabel('deuxieme composante principale')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJuPu60m4QmQ"
   },
   "source": [
    "Inspirez vous de ce programme pour appliquer l'ACP au jeu de données *digits*. Ensuite, (1) produire la figure de visualisation des données projetées, et (2) utiliser ces données projetées pour apprendre un $k$-ppv avec la norme $l2$, et tester ses performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NEKdUyeHejeG"
   },
   "outputs": [],
   "source": [
    "# A vous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "id2FVb3f6zfc"
   },
   "source": [
    "# Retour sur la matrice de covariance : standardisation nécessaire\n",
    "\n",
    "Nous avons déjà visualisé les données du jeu * Iris* en 2D, en visualisant dans de multiples graphiques la co-variation de chaque couple de variables (attributs). Rappelons maintenant que la matrice de covariance peut être un indicateur de ces co-variations, sans visualisation.\n",
    "\n",
    "Voici une façon simple de calculer la matrice de covariance de Iris. Comprenez les instructions au regard du cours, et visualisez le résultat après exécution des instructions. Quelles sont les variables qui co-varient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "Pi-mS5AT68sl",
    "outputId": "a9801ff1-6d59-4dae-9a82-95511b1a22a3"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "X,y = load_iris(return_X_y=True)\n",
    "C = np.dot(np.transpose(X), X)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2PBZoMY7ipA"
   },
   "source": [
    "Analysez attentivement cette matrice de covariance: quel est le couple de variables qui co-varient le plus ? \n",
    "\n",
    "Nous allons calculer cette même matrice après avoir standardisé les données pour les ramener à une variance unitaire: la standardisation est un processus classique qui permet de ramener tous les points autour de l'origine, de telle sorte que la variance globale de chaque variable soit unitaire. Pour chaque variable, $x$, et pour chaque ligne $i$, on standardise $x_i$ ainsi:\n",
    "$$x_i^s = \\frac{(x_i-\\bar{x})}{\\sigma(x)}$$\n",
    "\n",
    "Voici une façon de faire cette standardisation facilement avec sklearn: on utilise un opérateur qui va calculer dans un premier temps moyennes et écarts-types de chaque variable de l'échantillon, puis qui pourra alors transformer les données en conséquences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "dUKSTyk-9Lnl",
    "outputId": "b240f46e-e680-4bcc-b03c-946f455d4825"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "print(X[0:2,:]) # affichage des deux premiers exemples \"bruts\"\n",
    "scaler = StandardScaler() # le scaleur\n",
    "scaler.fit(X) # calcul des statistiques élémentaires\n",
    "Xstd = scaler.transform(X) # calcul de la standardisation de chaque donnée\n",
    "print(Xstd[0:2,:]) # affichage des deux premiers exemples standardisés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGoULy1wyTA8"
   },
   "source": [
    "Calculez maintenant la matrice de covariance de notre échantillon, après cette standardisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nN4tuuhxykry"
   },
   "outputs": [],
   "source": [
    "# a vous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKdJ3sp3yvy9"
   },
   "source": [
    "Quelles sont les deux variables qui co-varient le plus ? Est-ce que ce sont les mêmes que sans la normalisation préalable ?\n",
    "\n",
    "Dans le cas du jeu de données Iris, on constate que la standardisation change la donne. Dans certains jeux de données, le changement est spectaculaire.\n",
    "\n",
    "Appliquons maintenant ce même exercice aux jeu de données sur les vins, qui comporte 13 attributs d'échelle différentes: à partir de ce jeu de données (que l'on charge avec load_wine du package sklearn.datasets), \n",
    "indiquez les instructions permettant de calculer la matrice de covariance avant et après normalisation, puis déterminez dans chaque cas quel est le couple d'attributs qui varient le plus ensemble.\n",
    "\n",
    "1.  indiquez les instructions permettant de calculer la matrice de covariance avant et après normalisation, puis déterminez dans chaque cas quel est le couple d'attributs qui varient le plus ensemble.\n",
    "2.  comparez les performances d'un $k$-ppv sur les données transformées par ACP (réduction à deux dimensions), avec et sans standardisation ; dans quel cas l'estimation de l'erreur en généralisation est-elle meilleure ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4v2X23L2N_8"
   },
   "outputs": [],
   "source": [
    "# a vous"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tp2-M1ISD.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
